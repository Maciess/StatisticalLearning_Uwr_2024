---
title: "Statistical learning"
subtitle: "Report 3"
author: "Maciej Szczutko"
date: "`r Sys.Date()`"
output: pdf_document
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F ,fig.pos = "H", fig.dim = c(7,3), fig.align = "center", message = F, warning = F)
source("prettyMatrixPrinter.R")
source("helpers.R")
options(scipen = 1, digits = 2)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(mvtnorm)
library(dplyr)
library(lemon)
library(mvtnorm)
library(tidyr)
library(reshape2)
library(parallel)
library(bigstep)
library(glmnet)
library(SLOPE)
by_Row = 1
by_Col = 2
theme = theme(title = element_text(size = 9))
numCores  = round(parallel::detectCores() * .70)
```

# Properties of symmetric matrices 

Now we proof that the trace of symmetric matrix $X$ $tr(x) = \Sigma \lambda_i$. We use circular property of trace $\operatorname{tr}\left(\mathbf{A}^T \mathbf{B}\right)=\operatorname{tr}\left(\mathbf{A} \mathbf{B}^T\right)$. As the symmetric matrices $X$ always diagonalizable in $R$ by spectral theorem we can use spectral decomposition.

$$
\operatorname{tr}(\boldsymbol{X})=\operatorname{tr}\left(\mathbf{P} \boldsymbol{\Lambda} \mathbf{P}^T\right)=\operatorname{tr}\left(\boldsymbol{\Lambda} \mathbf{P}^T \mathbf{P}\right)=\operatorname{tr}(\boldsymbol{\Lambda})=\sum_{i=1}^n \lambda_i .
$$

# Properties of $X^TX$

Now let $X$ be the matrix of dimension $n x p$ ($p\leq n$). Then $X^TX$ is always semi-positive definite. Let $z \in \mathbb{R}^n$

$$
z^T\left(X^T X\right) z= \left(z^TX^T\right) X z =(X z)^T(X z)=\|X z\|_2^2 \geq 0.
$$

Now consider case where $p>n$. Then at least one eigen value must be equal to $0$. Note the fact the $rank(X) \leq n$. The $rank(X^TX) \leq \min \{ {rank(X) , rank(X^T)}\} \leq n$. We conclude the $X^TX$ is not full rank so it must be singular. Also $det(X^TX)=0$ implies $0$ is eigen value for this matrices. 


# Model selection criteria

Let's remark the AIC, BIC, RIC. In general we looking for model maximazing the value of information criterium. In linear model family is equivalent to minimizing below terms:

  1. AIC --  $R S S+2 \sigma^2 k$
  2. BIC --  $R S S+\sigma^2 k \log n$
  3. RIC --  $R S S+\sigma^2 2 k \log p$

$k$ means number of parameter used in models.

```{r}
AIC <- function(RSS, k, sigma_square = 1)
{
  return(RSS  + 2 * sigma_square * k)
}

BIC <- function(RSS, k, n, sigma_square = 1)
{
  return(RSS  +  sigma_square * log(n))
}

RIC <- function(RSS, k, p, sigma_square = 1)
{
  return(RSS  +  sigma_square * 2 * k * log(p))
}
```


```{r model-params}
rss_list <- c(1731, 730, 49, 38.9, 32, 29, 28.5, 27.8, 27.6, 26.6)
sample_size <- 100
p <- 10
```

```{r find-best-model}
AIC_score <- unlist(lapply(1:length(rss_list), function(i) {AIC(rss_list[i], i)}))
BIC_score <- unlist(lapply(1:length(rss_list), function(i) {BIC(rss_list[i], i, sample_size)}))
RIC_score <- unlist(lapply(1:length(rss_list), function(i) {AIC(rss_list[i], i, p)}))
```

After computing the value we can claim that the best model based on AIC is model `r which.min(AIC_score)`. For BIC model with `r which.min(BIC_score)` and RIC select model with `r which.min(RIC_score)` variables.

# Assuming the orthogonal design $\left(X^{\prime} X=I\right)$ and $n=p=10000$ calculate the expected number of false discoveries for AIC, BIC and RIC, when none of the variables is really important (i.e. $\left.p_0=p\right)$

## AIC

In such setup the probability of I type error is $P\left(X_i\right.$ is selected $\left.\mid \beta_i=0\right)=2(1-\Phi(\sqrt{2}))=0.16$
Then expected number of false discovery is $10000 * 0.16 = `r round(10000 * 0.16,0)`$ 

## BIC

$P\left(X_i\right.$ is selected $\left.\mid \beta_i=0\right)=2(1-\Phi(\sqrt{\log 1000})$

Then expected number of false discovery is $10000 * 0.0024 = `r round(10000 * 2*(1-pnorm(sqrt(log(10000)))),3)`$.

So the BIC is more restrictive and should make less false discoveries on average.


## RIC

$P\left(X_i\right.$ is selected $\left.\mid \beta_i=0\right)=2(1-\Phi(\sqrt{2 \log p}))$

Then expected number of false discovery is $10000 * 0.000018 = `r round(10000 * 2*(1-pnorm(sqrt(2*log(10000)))), 3)`$.

As was presented on lecture RIC is developed to avoid false discoveries even in large setup and keep expected value of false discovery below $1$.

# When would you use AIC ? BIC ? RIC ?

The AIC is equivalent for minimizing prediction error and can be use as initial step. The RIC should be use when we handle data with a large number of variables and the cost of making a mistake is large (e.g. medical industry).


## Ridge regression in orthogonal design

Ridge regression is a type of linear regression that includes a regularization term to prevent overfitting. It is achieved by modification LOSS function. Instead of minimizing L2 norm, we minize L2 + regularization term. Formally we looking for $\beta$ minimanize below terms:
$$
L_{\text {ridge }}(\hat{\beta})=\|y-X \hat{\beta}\|^2+\gamma\|\hat{\beta}\|^2 .
$$
for some constant value $\gamma$. $\gamma$ is used to manipulate bias and variance of $\hat{\beta}$.

Assuming the model is in the form 
$$
Y=X \beta+\varepsilon
$$ where $X$ is orthonormal and the errors has 0 $mean$ and $\sigma^2=1$. Estimator has following distribution;

$$
\hat{\beta} \sim N\left(\frac{1}{1+\gamma} \beta, \frac{1}{(1+\gamma)^2} I\right).
$$

so the theoretical bias $\hat{\beta}_i$ is $\frac{-\gamma}{1+\gamma} \beta_i$, variance $\frac{1}{(1+\gamma)^2}$. The MSE is
$$
E\left(\hat{\beta}_i-\beta_i\right)^2=\frac{\gamma^2}{(1+\gamma)^2} \beta_i^2+\frac{1}{(1+\gamma)^2}
$$

We want to use $\gamma$ which minimize the mean square error.
$$
E\|\hat{\beta}-\beta\|^2=\frac{\gamma^2}{(1+\gamma)^2}\|\beta\|^2+\frac{p \sigma^2}{(1+\gamma)^2} .
$$

From this we we find $\gamma_{o p t}=\frac{p}{\|\beta\|^2}$. Using $\gamma_{o p t}$ in $M S E$ formula we got $M S E\left(\gamma_{o p t}\right)=\frac{p\|\beta\|^2}{p+\|\beta\|^2}$.

In similar setup, if we use OLS estimator the estimator is unbiased. But the $MSE_{OLS}=p\sigma^2=p$. 

# Comparing prediction error ( Ridge vs OLS)


Assume we have dataset with 40 variables. We build model using OLS and Ridge. RSS are equal $4.5$ and $11.6$ respectively. For the ridge regression the trace of $X\left(X^{\prime} X+\gamma I\right)^{-1} X^{\prime}$ is equal to 32.

$\begin{gathered}P E_{\text {RR }}=11.6+2 \sigma^2 \cdot \operatorname{tr}(M)=11.6+64 \sigma^2 \\ P E_{O L S}=4.5+2 \sigma^2 p=4.5+80 \sigma^2\end{gathered}$

In this example PE from ridge should always be lower than OLS PE if $\sigma^2 > 0.44$.

# LASSO

Least Absolute Shrinkage and Selection Operator is another modification of OLS. We add penalty in terms of L1 norm of $\beta$

$L_{\text {lasso }}(\hat{\beta})=\sum_{i=1}^n\left(y_i-x_i^{\prime} \hat{\beta}\right)^2+\lambda \sum_{j=1}^p\left|\hat{\beta}_j\right|$.

The LASSO select important variables by setting some coefficients of $\beta$ to $0$. The solution of the problem might be express in terms of OLS estimator. Let us define shrinkage operator:
$$
\eta_\lambda(x)=\operatorname{sign} x(|x|-\lambda)_{+}
$$

The coefficients of LASSO estimator are given by $\hat{\beta}_{LASSO}=\eta_{\lambda}\left(\hat{\beta}_{OLS}\right)$. 

Recall that OLS estimator is unbiased. So if true $\beta_i=0$ then  $\hat{\beta}^{OLS}_i \sim N\left(0,\sigma^2\right)$. Now we will calculate the probability of single false discovery in $\beta$ vector.

$P\left(\text{ I type error }\right) = P\left(\left|\hat{\beta}_{i}^{OLS}\right|>\lambda | \beta_i=0\right) =  P\left(\frac{\left|\hat{\beta}_{i}^{OLS}\right|}{\sigma} > \frac{\lambda}{\sigma} | \beta_i=0\right) = 2\left(1-\Phi\left(\frac{\lambda}{\sigma}\right)\right)$.

Assuming the number of non zero coefficients within $\beta$ is $p_0$ the expected number of false discovery is $2p_0\left(1-\Phi\left(\frac{\lambda}{\sigma}\right)\right)$ and depend on choice of the $\lambda$.

# Adaptive LASSO

Adaptive LASSO is extension where instead of use L1 norm we us L1-weighted norm.

$L_{\text {lasso }}(\hat{\beta})=\sum_{i=1}^n\left(y_i-x_i^{\prime} \hat{\beta}\right)^2+\lambda \sum_{j=1}^p w_i \left|\hat{\beta}_j\right|$.

The weights can be set to $w_i = \frac{1}{|\hat{\beta}_i^{RIDGE}|}$. This can be interpret as follow: the ridge shrink the coefficients. If the estimated coefficients from Ridge is close to $0$ then the penalty become large and likely aLASSO zero this coefficients. Roughly speaking RIDGE detect initial importance of features.

This give direct instruction how to calculate aLASSO with **glmnet**. 



```{r setup-adaptive-lasso}
set.seed(42)
n = 200
p = 30
k = 20
sig = 1
cov_matrix <- matrix(0, nrow = p, ncol = p)
diag(cov_matrix) <- sqrt(1/n)

beta_true = c(rep.int(3,k), rep.int(0,p-k))
X = rmvnorm(n = n, mean = rep(0, p), sigma = cov_matrix)
XB <- X %*% beta_true
y = XB + rnorm(n)
```

```{r how-to-lasso, echo=T, include=T}
#Calculate RIDGE estimator
lambda_MSE_min <- cv.glmnet(X,y, alpha = 0)$lambda.min
ridge <- glmnet(X, y, alpha = 0, lambda = lambda_MSE_min, intercept = F)
beta_ridge <- coef(ridge)[-c(1)] # (-1 to exclude Intercept)
adaptive_lasso <- glmnet(X,y, alpha = 1, penalty.factor =  1/abs(beta_ridge))
```

Using previous notation we can write explicitly the coeficients
$\hat{\beta}_{LASSO}=\frac{1}{w}\eta_{\lambda}\left(w \hat{\beta}_{OLS}\right)$ where $w=(w_1,w_2, \ldots, w_p)$.

**Note**: $w_i$ are positive.

The ordinary least squares estimator of $\beta_1$ underr the orthognal design ( $\mathrm{X}^{\prime} \mathrm{X}=\mathrm{I}$ ) is equal to 3 and the LASSO estimator of this parameter is equal to 2. What is the value of the adaptive LASSO estimator of $\beta_1$ if we use the same value of $\lambda$ and the weight for $X_1$ is $w_1=1 / 4$.

Then we find $\lambda=1$ and using above formula we calculate $4\eta_{\lambda=1}\left(\frac{1}{4} * 3\right)=0.$


# James-Stein estimator and Prediction Error in Multiple Regression

We analyze data with observation of 300 genes for 210 patients. We standarize data and later add previous mean vector. Here we will use two estimator JS estimator shrinking toward zero and shrinking to common mean. First estimator is  $\hat{\mu}_c=c_{JS} \hat{\mu}_{M L E}$ where $c_{J S}=1-\frac{(p-2) \sigma^2}{\|\hat{\mu}_{M L E}\|^2}$.

The second one is  $\hat{\mu}_d=(1-d) \hat{\mu}_{M L E}+d \bar{{\mu}_{M L E}}$ where $d_{J S}=\frac{p-3}{p-1} \frac{\sigma^2}{\operatorname{Var}(\hat{\mu}_{M L E})}$.

```{r read-data}

data <- load("Lab3.Rdata")
X <- as.matrix(xx)
numOfColumns <- dim(X)[1]
numOfObservation <- dim(X)[2]
geanes_mean <- colMeans(X)
X.scaled <- scale(X, center = T, scale  = apply(X, 2, sd, na.rm=T))
X.centered <- sweep(X.scaled, 2, geanes_mean, "+") - 10
# is it ok?
```

```{r estimate-using-only-five}
X.first_five <- head(X.centered, n = 5)
X.all_except_first_five <-tail(X.centered, n = numOfObservation - 5)
```


```{r compute-JS-correction}

y_mle_based_on_5 <- colMeans(X.first_five)
y_js_based_on_5 <- compute_js_estimator(y_mle_based_on_5, sigma_square = 1/5)
y_js_common_based_on_5 <- compute_js_estimator_common_mu(y_mle_based_on_5, sigma_square = 1/5)
```


```{r compute-JS-correction-tail}
mu <- colMeans(X.all_except_first_five)
```

```{r}
df <- data.frame(mu_true = mu, MLE = y_mle_based_on_5, JS_Origin_shrinkage = y_js_based_on_5, JS_common_mean_shrinkage = y_js_common_based_on_5)
df_melted <- melt(df, id.vars = "mu_true", variable.name = "Estimator", value.name = "Value")

```
```{r}
ggplot(data = df_melted, aes(x= Value, y = mu_true, color = Estimator))+
  geom_point(size = 1)+
  geom_abline(aes(intercept= 0,slope= 1), color = "#142224", style = "dashed", size = 0.5)
```

I can't infer anything from such a chart. I don't know what it would help me with in practice. 

```{r MSE-for-esitimator}

mse_mle <- norm(y_mle_based_on_5 - mu, type = '2')^2
mse_js <- norm(y_js_based_on_5 - mu, type = '2')^2
mse_js_common <- norm(y_js_common_based_on_5 - mu, type = '2')^2
```

| $\hat{\mu}$  |  MSE |
|---|---|
| MLE  | $`r round(mse_mle,3)`$  |
| JS_Origin_Shrinkage  |`r round(mse_js,3)`  |
| JS_Common_Mean_Shrinkage  | `r round(mse_js_common,3)`  |

From above table we see the estimator works as were designed. We achieved lower MSE for both JS estimator. 


**Note** $\sigma^2$ used for JS constant computation is $0.2$ because sample comes from distribution $N(0, \sqrt{\frac{1}{5}})$.

## Prediction error estimation in linear model under orthogonal design. 

We consider linear model $Y=\beta X+\epsilon$, where $X$ is a $n \times p$ plan matrix, $\epsilon \sim N\left(0_n, \sigma^2 I_{n \times n}\right)$ is a vector representing random noise and $\beta \in R^n$ is a vector of parameters. Let $\hat{\beta}$ be the estimate of $\beta$ based on $Y$ and some subset $X_{\tilde{p}}$ of $X$ columns. We discuss some criteria of $X_{\tilde{p}}$ selection.


Least squares estimator $\hat{\beta}_{L S}$ minimizes the error in the training sample (a.k.a. residual sum of squares) $R S S=\|Y-\hat{Y}\|^2$, where $\hat{Y}=X \hat{\beta}$ and $Y$ is the response used to fit the model. It might seem to be a good idea to pick $X_{\tilde{p}}$ resulting in smallest value of $R S S$. Indeed it doesn't make sense when comparing models with different number of columns (which we want to do), because $R S S$ never increases when we add more variables. The thing we should minimize instead is the prediction error defined as
$$
P E=E\left(\hat{Y}-Y^*\right)^2,
$$

where $Y^*=X \beta+\epsilon^*$ and $\epsilon^*$ is a new noise, random and independent of that in training sample. The expression can be rewritten as follows:

$\begin{gathered}P E=E\left\|X \hat{\beta}+\epsilon^*-X \beta\right\|^2=E \sum_{i=1}^n\left(X(\beta-\hat{\beta})+\epsilon^*\right)^2=E \sum_{i=1}^n\left[(X(\beta-\hat{\beta}))^2+2 X(\beta-\hat{\beta}) \epsilon^*+\left(\epsilon^*\right)^2\right]= \\ E \sum_{i=1}^n\left[(X(\beta-\hat{\beta}))^2+2 \sum_{i=1}^n E[X(\beta-\hat{\beta})] E\left[\epsilon^*\right]+\sum_{i=1}^n E\left(\epsilon^*\right)^2\right]=E \sum_{i=1}^n(X(\beta-\hat{\beta}))^2+0+n \sigma^2=E\|X(\beta-\hat{\beta})\|^2+n \sigma^2\end{gathered}$


Stein's identity allows us to replace the first term with something called Stein's Unbiased Risk Estimator (exact form depends on the used estimator). If we use least squares estimator for parameters $\hat{\beta}=\left(X^T X\right)^{-1} X^T Y$, then $\hat{Y}=X \hat{\beta}=X\left(X^T X\right)^{-1} X^T Y=M Y$ and by Stein's identity

$$
E\|X(\beta-\hat{\beta})\|=R S S+\operatorname{tr}(M) \sigma^2-n \sigma^2 \Longrightarrow P E=R S S+\operatorname{tr}(M) \sigma^2 .
$$

Trace of $M$ is $p$ if it's full rank. If $\sigma^2$ is unknown it should be replaced with its unbiased estimator $s^2=\frac{R S S}{n-p}$. Another way of estimating the prediction error is by this formula which makes it easy to compute result of leave-one-out cross validation:



$$
\hat{P} E=\sum_{i=1}^n\left(\frac{Y_i-\hat{Y}_i}{1-M_{i, i}}\right)^2,
$$

where $M=X\left(X X^T\right)^{-1} X^T$ is a matrix of projection onto $\operatorname{Lin}(X)\left(X\right.$ denotes here $X_{\tilde{p}}$, a subset of $\left.X\right)$.



```{r setup_zad1, fig.height= 2.5, fig.width= 6, cache=TRUE}
set.seed(42)
# Setup parameters
n = 1000
p = 950
k = 5
sig = 1

cov_matrix <- matrix(0, nrow = p, ncol = p)
diag(cov_matrix) <- sqrt(1/n)

beta_true = c(rep.int(3,k), rep.int(0,p-k))
X = rmvnorm(n = n, mean = rep(0, p), sigma = cov_matrix)
XB <- X %*% beta_true
# Define experiment and functions  
get_RSS = function(true, pred){sum((true-pred)^2)}

task1_single_p = function(p, X, beta_true, sig){
  # SETUP ----------
  Y_train = XB + rnorm(nrow(X), 0, sig)
  Xp = X[,1:p]
  constTerm = solve(t(Xp) %*% Xp) %*% t(Xp)
  M = Xp %*% constTerm
  beta_LS = constTerm %*% Y_train
  Y_pred = Xp %*% beta_LS
  # PREDICTION ERRORS ---------
  RSS = sum((Y_train - Y_pred)^2)
  PE_1 = sum((Xp %*% beta_true[1:p] - Y_pred)^2) + n*sig^2 # true PE
  PE_2 = RSS + 2*p*sig^2                              # SURE known sigma
  PE_3 = RSS + 2*p*(RSS/(n-p))                        # SURE unknown sigma
  PE_4 = sum(((Y_train-Y_pred)/(1-diag(M)))^2)        # LOO CV
  # RESULTS ---
  return(c(p, RSS, PE_1, PE_2, PE_3, PE_4))
}

task1_all_ps = function(ps, X, beta_true, sig){
  results <- sapply(ps, task1_single_p, X, beta_true, sig)
  results = t(as.data.frame(results))
  rownames(results) = NULL
  colnames(results) = c('p','RSS', 'PE1','PE2','PE3','PE4')
  return(results)
}

task1_replicate = function(ps, X, beta_true, sig = 1, iters = 30){
  ## doParallel
  cl <- makeCluster(numCores)
  on.exit(stopCluster(cl))
  clusterExport(cl, varlist = c("ps", "X", "beta_true", "sig", "task1_all_ps", "task1_single_p", "XB","n", "p"))
  trials <- parLapply(cl, 1:iters, function(iter) { task1_all_ps(ps, X, beta_true, sig) })
  return(trials)
}

average_dtfs = function(trials){
  mean_results = array(unlist(trials), c(dim(trials[[1]]), length(trials))) %>% apply(c(1,2), mean) %>% data.frame()
  colnames(mean_results) <- colnames(trials[[1]])
  return(mean_results)
}

ps = c(2, 5, 10, 100, 500, 950)
task1_gathered = task1_replicate(ps, X, beta_true)
task1_avg = average_dtfs(task1_gathered)
```

The table below show numeric results of experiment replicated 30 times, where `PE1`, `PE2`, `PE3`, `PE4` are respectively: true prediction error, SURE with known $\sigma^2$, SURE with unknown $\sigma^2$ and LOO cross-validation estimator.

```{r task1_show_avg}
task1_avg %>% kbl(caption = "Task 1 - numeric results averaged over 30 reps (seed = 42)", digits = 2, booktabs = T) %>% kable_styling(latex_options = c("striped", "HOLD_position"))
```

From table above we can see why we can't use RSS as model selection criterion. Even if we adding dummy column we can fit better to data, but we don't bring any useful information for model. 

## Boxplots (Prediction Error)

```{r task1_boxplots, fig.width= 12, fig.height= 5}
get_PE_diffs = function(df_lst, PE_col){
  ps = unique(df_lst[[1]][,"p"])
  r = sapply(df_lst, function(df){abs(df[,"PE1"] - df[,PE_col])}) %>% 
    t() %>% as.data.frame()
  colnames(r) = ps
  valname = paste0(PE_col)
  return(r %>% gather(key = "p", value = !!valname))
}
  
PE_diffs_lst = lapply(list("PE2", "PE3", "PE4"), function(pe){get_PE_diffs(task1_gathered, pe)})
PE_diffs_merged = data.frame(
  p = factor(as.numeric(PE_diffs_lst[[1]]$p)),
  PE2 = PE_diffs_lst[[1]]$PE2,
  PE3 = PE_diffs_lst[[2]]$PE3,
  PE4 = PE_diffs_lst[[3]]$PE4
)

PE_diffs_long = gather(PE_diffs_merged, key = estimator, value = difference, c(2,3,4))

big_boxplots = ggplot(PE_diffs_long, aes(x = p, fill = estimator, y = difference)) + 
  geom_boxplot() +
  labs(title = "Absolute difference between estimated and true PE",
       subtitle = "30 replications") + theme

zoom_boxplots = ggplot(PE_diffs_long %>% filter(as.numeric(p) <= 5), aes(x = p, fill = estimator, y = difference)) + 
  geom_boxplot() +
  labs(title = "Absolute difference between estimated and true PE",
       subtitle = "30 replications, cut to p < 950")  + theme

zoom_underr_500_boxplots = ggplot(PE_diffs_long %>% filter(as.numeric(p) <= 4), aes(x = p, fill = estimator, y = difference)) + 
  geom_boxplot() +
  labs(title = "Absolute difference between estimated and true PE",
       subtitle = "30 replications, cut to p < 500") + theme

grid_arrange_shared_legend(big_boxplots, zoom_boxplots, zoom_underr_500_boxplots, ncol = 3, nrow = 1)
```

## Model selection and regularization in multiple regression models.

```{r, generate-matrix}

set.seed(42)
# Setup parameters
n = 1000
p = 950
k = 20
sig = 1

cov_matrix <- matrix(0, nrow = p, ncol = p)
diag(cov_matrix) <- 1/sqrt(n)

beta_true = c(rep.int(6,k), rep.int(0,p-k))
X = rmvnorm(n = n, mean = rep(0, p), sigma = cov_matrix)
eps <- rnorm(n)
y <- X %*% beta_true + eps
```

We will use same setup as before. The only change is the $\beta$ vector which is now $\beta_1=\ldots=\beta_k=6, \beta_{k+1}=\ldots=\beta_p=0$ with $k=20$. Note the fact this time we make assumption we do not know nothing about the data. We will perform selection and regularization methods feeding the model with full matrix $X$ ($950$ predictors!). Following method will be used:


## mBIC2

mBIC is designed to handle sligthly different case (when the number of parameters is greater than observation), but mBIC2 should adopt well to data with unknown sparisty.

We will use implementation from **bigstep** combined with stepwise selection method.
```{r, mBiC2-selecte-model}
data <- bigstep::prepare_data(y, X)
model_based_mBIC2 <- bigstep::stepwise(data, crit = bigstep::mbic2)

selectedVariables <-  sort(as.numeric(model_based_mBIC2$model))
coefs <- summary(model_based_mBIC2)$coefficients[, 1]

b_hat <- rep(0, length(beta_true))
b_hat[as.numeric(model_based_mBIC2$model)] <- coefs[-c(1)]
```

```{r metrics-calculation-mBIC2}

norm_square <- function(vector)
{
  norm(vector, type = "2")^2
}


vars <- rep(F, length(beta_true))
vars[as.numeric(model_based_mBIC2$model)] <- T



MSE <- norm_square(b_hat - beta_true)
MSE2 <- norm_square(X %*% (b_hat - beta_true))

FP=sum(vars & (beta_true == 0))
FN=sum(!vars & (beta_true > 0))
TP=sum(vars & (beta_true > 0))
Reject=length(model_based_mBIC2$model)
FDP <- FP/(max(Reject, 1))
TPP <- TP/ sum(beta_true > 0)
```


The method choose $`r length(selectedVariables)`$ variables with indexes `r selectedVariables`. $\|\hat{\beta}-\beta\|^2$ = $`r MSE`$,
$\|X(\hat{\beta}-\beta)\|^2$ = `r MSE2`. FDP `r FDP` and power $`r TPP`$.

## Ridge with the tunning parameter selected by cross validation

First we need to estimate the $\lambda$ by CV. We will use k-folds provided in **glmnet** library.

```{r ridge-lambda-estimation, fig.dim=c(6,3.5)}

cv_fit <- cv.glmnet(X, y, alpha = 0, standardize = F, intercept=F)
optimal_lambda <- cv_fit$lambda.min
plot(cv_fit)
```
The $\lambda$ estimated through k-folds CV is $`r round(optimal_lambda,4)`$. From plot we can see that we might as well use some other values like $exp(0)$ or lower without incorporating model performance. Note we analyse here just one particular design matrix.

```{r model-ridge}
ridge_model <- glmnet(X, y, alpha = 0, lambda = optimal_lambda, intercept=F, standardize=F)
b_ridge <- ridge_model$beta
```


```{r calculate-metric-for-ridge}
MSE_ridge <- norm_square(b_ridge - beta_true)
MSE2_ridge <- norm_square(X %*% (b_ridge - beta_true))
```
$\|\hat{\beta}-\beta\|^2$ = $`r round(MSE_ridge,4)`$, $\|X(\hat{\beta}-\beta)\|^2$ = `r round(MSE2_ridge,4)`.

## LASSO with the tuning parameter selected by cross-validation

For LASSO we will use to $\lambda$ computed via cv.glmnet. Lambda.min : $\lambda$ of minimum mean cross-validated error. lambda.1se : largest value of $\lambda$ such that error is within 1 standard error of the crossvalidated errors for lambda.min.

```{r lasso-model}
lambdas_to_try <- 10^seq(-3, 3, length.out = 100)
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = F, intercept=F)

lambda_lasso <- lasso_cv$lambda.min
lambda_lasso_1se <- lasso_cv$lambda.1se
plot(lasso_cv)
```

```{r lasso-models}
model_lasso <- glmnet(X, y, standardize = F, intercept=F, alpha=1, lambda = lambda_lasso)
b_lasso <- model_lasso$beta

model_lasso_1se <- glmnet(X, y, standardize = F, intercept=F, alpha=1, lambda = lambda_lasso_1se)
b_lasso_1se <- model_lasso_1se$beta
```

```{r calculate-metric-for-lasso}
MSE_lasso <- norm_square(b_lasso - beta_true)
MSE2_lasso <- norm_square(X %*% (b_lasso - beta_true))

MSE_lasso_1se  <- norm_square(b_lasso_1se - beta_true)
MSE2_lasso_1se  <- norm_square(X %*% (b_lasso_1se - beta_true))
```

```{r calculate-metric-for-lasso-discoveires}
lasso_discoveries <- get_FDP_and_TPP(b_lasso, beta_true)
lasso_1se_discoveries <- get_FDP_and_TPP(b_lasso_1se, beta_true)

```

For LASSO model with lambda.min $\|\hat{\beta}-\beta\|^2$ = $`r round(MSE_lasso, 3)`$, $\|X(\hat{\beta}-\beta)\|^2$ = `r round(MSE2_lasso, 3)`.
For LASSO model with lambda.1se $\|\hat{\beta}-\beta\|^2$ = $`r round(MSE_lasso_1se, 3)`$, $\|X(\hat{\beta}-\beta)\|^2$ = `r round(MSE2_lasso_1se, 3)`.

For this case we can see the 1se version has worse score in case of square errors but lower the false discovery proportion. Also power is preserved.


## LASSO with the tuning parameter $\lambda=\Phi^{-1}\left(1-\frac{0.1}{2 p}\right)$

```{r lasso-model-with-given-lambda}
lambda_custom <- pnorm(1-0.1/(2*p))/n
model_lasso_custom <- glmnet(X, y, standardize = F, intercept=F, alpha=1, lambda = lambda_custom)
b_laso_custom <- model_lasso_custom$beta
```


For this setup the $\lambda=`r round(lambda_custom,3)`$. The value looks like special value for Bonferroni correction. Indeed if we recall formula for flase discovery probability from exercise ($2\left(1-\Phi\left(\frac{\lambda}{\sigma}\right)\right)$) and use given $\lambda$ we got $\frac{\alpha}{p}$. So this value is use to force LASSO to use Bonferroni correction.

```{r custom-lambda-metric}
MSE_lasso_custom <- norm_square(b_laso_custom - beta_true)
MSE2_lasso_custom <- norm_square(X %*% (b_laso_custom - beta_true))
lasso_custom_discoveries <- get_FDP_and_TPP(b_laso_custom, beta_true)

```



## SLOPE with the $\mathrm{BH}$ sequence of the tuning parameters $\lambda_i=\Phi^{-1}\left(1-\frac{0.1 i}{2 p}\right)$

No we will change canonical loss function. Instead using L2 penalty we will try to minimazie L1 sorted norm. 
$$
\hat{\beta}=\operatorname{argmin}_{b \in \mathbb{R}^p} \frac{1}{2}\|y-X b\|_{\ell_2}^2+\sum_{i=1}^p \lambda_i|b|_{(i)} .
$$

The estimator can be compute with SLOPE package. 

```{r SLOPE, cache=TRUE}
i_vec <- 1:p
lambda_slope <- pnorm(1-(0.1*i_vec)/(2*p))/n
model_slope <- SLOPE(X, y, lambda = lambda_slope, alpha=0.5/sqrt(n) ,intercept = F)
```

```{r SLOPE-metrics}
b_slope <- coef(model_slope)
MSE_SLOPE <- norm_square(b_slope - beta_true)
MSE2_SLOPE <- norm_square(X %*% (b_slope - beta_true))
b_slope_discoveries <- get_FDP_and_TPP(b_slope, beta_true)
```



|   | $\|\hat{\beta}-\beta\|^2$  | $\|X(\hat{\beta}-\beta)\|^2$  | FDP  | TPP  |
|---|---|---|---|---|
| mBIC2  | `r round(MSE, 3)`  | `r round(MSE2, 3)`  |  `r FDP` | `r TPP`  |
| Ridge  | `r round(MSE_ridge, 3)`  | `r round(MSE2_ridge, 3)`  |  - | -  |
| LASSO  | `r round(MSE_lasso, 3)`  | `r round(MSE2_lasso, 3)`  |  `r round(lasso_discoveries["FDP"],3)` |  `r round(lasso_discoveries["TPP"],3)` |
| LASSO_1se  | `r round(MSE_lasso_1se, 3)`  | `r round(MSE2_lasso_1se, 3)`  | `r round(lasso_1se_discoveries["FDP"],3)`  | `r round(lasso_1se_discoveries["TPP"],3)`  |
| LASSO_custom  | `r round(MSE_lasso_custom, 3)`  | `r round(MSE2_lasso_custom, 3)`  | `r round(lasso_custom_discoveries["FDP"],3)`  | `r round(lasso_custom_discoveries["TPP"],3)`  |
| SLOPE  | `r round(MSE_SLOPE, 3)`  | `r round(MSE2_SLOPE, 3)`  |  `r round(b_slope_discoveries["FDP"], 3)` | `r round(b_slope_discoveries["TPP"], 3)`  |


The mBIC2 is the best among all tested method. It was able to correctly detect all true signals from data and make no false discovery. Ridge regression fails in such high dimensional setup.
The difference between LASSO and LASSO_1se is as mentioned before. The second version lower the FDP.

I don't know what is natural interpretation of SLOPE - mainly how to interpret the sorted L1 norm. I try to get some intuition from orignal paper but Im fail. I know it has good properties like convexity but in practice I don't know when I would use this method. Also the yielded results seems to be very strange.


**Note** Without replication.
